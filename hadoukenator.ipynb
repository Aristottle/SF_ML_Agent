{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "Only need to do this once for your environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install retro\n",
    "!pip install gym gym-retro\n",
    "\n",
    "# We have to downgrade gym in order to preserve retro's functionality\n",
    "!pip install gym==0.21.0\n",
    "\n",
    "# A library we use to preprocess the frame for training\n",
    "!pip install opencv-python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the ROMs to retro\n",
    "After retro is installed to your environment, you also have to add the Street Fighter ROMs to retro!\n",
    "You do so by navigating to the location of your ROM in the terminal while your environment is activated\n",
    "and running this command:\n",
    "`python -m retro.import .`\n",
    "\n",
    "ROM can be downloaded [here](https://wowroms.com/en/roms/sega-genesis-megadrive/street-fighter-ii-special-champion-edition-europe/26496.html). Just put it in a ROMs folder and do `extract here`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SF2 For Gym-Retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retro just allows us to interface with the game ROM through the emulator\n",
    "import retro\n",
    "# Import time so we can control the speed of the game manually\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment For Training\n",
    "\n",
    "### What we're going to do\n",
    "- Preprocess Observations (condense the info passed to the agent) - grayscale, frame delta, resize the frame so we have less pixels.\n",
    "- Filter the action - parameter\n",
    "- Custom reward function. For our purposes we are just going to do the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base environment classe for a wrapper\n",
    "from gym import Env\n",
    "# The space shapes\n",
    "from gym.spaces import MultiBinary, Box\n",
    "# Helper libraries for preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom environment\n",
    "class StreetFighter(Env):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Specify action and observation spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        # Start an instance of the game\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Gets the first frame\n",
    "        obs = self.game.reset()\n",
    "        # Preprocess the data\n",
    "        obs = self.preprocess(obs)\n",
    "        # At the start, just make the prev frame the first frame as well\n",
    "        self.previous_frame = obs\n",
    "        # Create an attribute to hold the score delta\n",
    "        self.score = 0  \n",
    "              \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, obs):\n",
    "        # Grayscale the frame\n",
    "        grayed = cv2.cvtColor(obs, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize the frame\n",
    "        resized = cv2.resize(grayed, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        # Add a color channels value to resized\n",
    "        processed = np.reshape(resized, (84, 84, 1))\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    # Frame-step\n",
    "    def step(self, action):\n",
    "        # Take a step\n",
    "        obs, def_reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        # Frame delta\n",
    "        # frame_delta = obs - self.previous_frame\n",
    "        frame_delta = obs\n",
    "        self.previous_frame = obs\n",
    "        # Reshape the reward function\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages we need to do our reinforcement learning\n",
    "# Pytorch is the base ML framework\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "# Stable-Baselines has a lot of stuff for reinforcement learning specifically\n",
    "!pip install stable-baselines3[extra]\n",
    "# Optuna is used to help us tune our hyperparameters efficiently\n",
    "!pip install optuna"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "This isn't the actual training, this is just an easy way for us to determine the best hyperparam values for PPO in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Used for hyper-param optimization\n",
    "import optuna\n",
    "# PPO algorithm for RL\n",
    "from stable_baselines3 import PPO\n",
    "# Used to evaluate how well the model is performing on this environment\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Used for logging\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize our frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'\n",
    "\n",
    "# Function to return the test hyperparams\n",
    "def OptimizePPO(trial: optuna.trial.Trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma': trial.suggest_loguniform('gamma', .8, .9999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-7, 1e-6),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', .1, .4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', .8, .99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop and return mean reward\n",
    "def OptimizeAgent(trial: optuna.trial.Trial):\n",
    "    try:\n",
    "        params = OptimizePPO(trial)\n",
    "\n",
    "        # Create environment\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        \n",
    "        # Create the model\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **params)\n",
    "        # Train. Obviously, more timesteps means better results. 100000 would be the ideal number but\n",
    "        # we will just do 30000 for now so we can see results quickly.\n",
    "        model.learn(total_timesteps=300000, tb_log_name='LR_TEST')\n",
    "        \n",
    "        # Evaluate the model. n_eval_episodes is the number of games the model plays. Should be like 20 to 30.\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        env.close()\n",
    "        \n",
    "        # Save the model so we don't have to do this again\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    \n",
    "    except Exception as e:\n",
    "        if env != None: env.close()\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# Optimize our boy. n_trials would ideally be higher (like 100), but we want results so for now we will do much less (like 10).\n",
    "# Also, if we install the retro wrapper that allows parallelization we could do more than 1 job at a time and\n",
    "# train MUCH faster, so should prob look into that.\n",
    "study.optimize(func=OptimizeAgent, n_trials=5, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best params to use in the actual training. This is super handy!\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best trial we had in our testing\n",
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rates = [5e-7, 5e-6, 5e-5, 5e-4, 5e-3]\n",
    "\n",
    "for rate in range(len(l_rates)):\n",
    "    # Investigate learning rates\n",
    "    params = {\n",
    "        'n_steps': 4096,\n",
    "        'gamma': .9,\n",
    "        'learning_rate': l_rates[rate],\n",
    "        'clip_range': .25,\n",
    "        'gae_lambda': .9\n",
    "    }\n",
    "\n",
    "    print(\"Testing l_rates[{}] : {}\".format(rate, l_rates[rate]))\n",
    "\n",
    "    # Create environment\n",
    "    env = StreetFighter()\n",
    "    env = Monitor(env, LOG_DIR)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    \n",
    "    # Create the model\n",
    "    model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **params)\n",
    "    # Train. Obviously, more timesteps means better results. 100000 would be the ideal number but\n",
    "    # we will just do 30000 for now so we can see results quickly.\n",
    "    model.learn(total_timesteps=300000, tb_log_name='LR_TEST')\n",
    "    \n",
    "    # Evaluate the model. n_eval_episodes is the number of games the model plays. Should be like 20 to 30.\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    env.close()\n",
    "    \n",
    "    # Save the model so we don't have to do this again\n",
    "    SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(rate))\n",
    "    model.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load up our best trial and see what he can do with real training! This is why we are saving them.\n",
    "model = PPO.load(os.path.join(OPT_DIR, 'trial_10_best_model.zip'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Our Training Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainingAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "\n",
    "callback = TrainingAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model!\n",
    "### Viewing the performance with tensorboard\n",
    "To do so, just cd into the logs directory and run this: `tensorboard --logdir=.` . Tensorboard will run on a localhost that you can view in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we haven't run a study, use these (we found them to be best)\n",
    "params = {'n_steps': 2570.949, 'gamma': 0.906, 'learning_rate': 2e-07, 'clip_range': 0.369, 'gae_lambda': 0.891}\n",
    "params['n_steps'] = int(np.round(params['n_steps'] / 64)) * 64\n",
    "# params['learning_rate'] = 5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best params from the study\n",
    "params = study.best_params\n",
    "# Correct n_steps to be a factor of the batch size (64)\n",
    "params['n_steps'] = int(np.round(study.best_params['n_steps'] / 64)) * 64\n",
    "# Let's use a lower learning rate\n",
    "# params['learning_rate'] = 5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Create the model with the optimal params\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Only pick 1 (or no) load to do (duh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our best performing model from hyperparam optimization for a head start\n",
    "model.load(os.path.join(OPT_DIR, 'trial_{}_best_model.zip'.format(int(9))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, 'akash','trial_{}_best_model.zip'.format(int(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f50bea09a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the best performance we saw.\n",
    "model.load('./train/best_model_1900000.zip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f4bf3addf0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the best performance we saw.\n",
    "model.load('./train/from_tut/best_model_5460000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously, more timesteps means better performance. Like 5000000 would be a good number.\n",
    "model.learn(total_timesteps=2000000, callback=callback, reset_num_timesteps=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well the model works\n",
    "mean_reward, reward_stdev = evaluate_policy(model, env, n_eval_episodes=5, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch Our Boy Work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to close the env before starting a new one.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.]\n",
      "[100.]\n",
      "[300.]\n",
      "[500.]\n",
      "[300.]\n",
      "[500.]\n",
      "[1000.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[10000.]\n",
      "[500.]\n",
      "[1000.]\n",
      "[500.]\n",
      "[500.]\n",
      "[100.]\n",
      "[500.]\n",
      "[100.]\n",
      "[1500.]\n",
      "[1000.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[500.]\n",
      "[300.]\n",
      "[500.]\n",
      "[500.]\n",
      "[1000.]\n",
      "[900.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[100.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[1000.]\n",
      "[10000.]\n",
      "[100.]\n",
      "[500.]\n",
      "[500.]\n",
      "[500.]\n",
      "[300.]\n",
      "[100.]\n",
      "[500.]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RetroEnv' object has no attribute 'em'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mu:\\PythonProjects\\SF2_AI\\hadoukenator.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Only play one game\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m game \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_games):\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Resest the env\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Will tell us if we've died or beaten the game\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py:38\u001b[0m, in \u001b[0;36mVecFrameStack.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]]:\n\u001b[1;32m---> 38\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mreset()  \u001b[39m# pytype:disable=annotation-type-mismatch\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstacked_obs\u001b[39m.\u001b[39mreset(observation)\n\u001b[0;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m observation\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:74\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvObs:\n\u001b[0;32m     73\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 74\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     75\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:84\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected you to pass keyword argument \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m into reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_reset_info[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m---> 84\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mu:\\PythonProjects\\SF2_AI\\hadoukenator.ipynb Cell 42\u001b[0m in \u001b[0;36mStreetFighter.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Gets the first frame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Preprocess the data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(obs)\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\retro\\retro_env.py:195\u001b[0m, in \u001b[0;36mRetroEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_state:\n\u001b[1;32m--> 195\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mem\u001b[39m.\u001b[39mset_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_state)\n\u001b[0;32m    196\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayers):\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mem\u001b[39m.\u001b[39mset_button_mask(np\u001b[39m.\u001b[39mzeros([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_buttons], np\u001b[39m.\u001b[39muint8), p)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RetroEnv' object has no attribute 'em'"
     ]
    }
   ],
   "source": [
    "# The game loop\n",
    "num_games = 5\n",
    "# Only play one game\n",
    "for game in range(num_games):\n",
    "    # Resest the env\n",
    "    obs = env.reset()\n",
    "    # Will tell us if we've died or beaten the game\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Take a breather (60fps)\n",
    "        # time.sleep(1/90)\n",
    "        if (reward != 0): print(reward)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
