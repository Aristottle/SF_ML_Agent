{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "Only need to do this once for your environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install retro\n",
    "!pip install gym gym-retro\n",
    "\n",
    "# We have to downgrade gym in order to preserve retro's functionality\n",
    "!pip install gym==0.21.0\n",
    "\n",
    "# A library we use to preprocess the frame for training\n",
    "!pip install opencv-python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the ROMs to retro\n",
    "After retro is installed to your environment, you also have to add the Street Fighter ROMs to retro!\n",
    "You do so by navigating to the location of your ROM in the terminal while your environment is activated\n",
    "and running this command:\n",
    "`python -m retro.import .`\n",
    "\n",
    "ROM can be downloaded [here](https://wowroms.com/en/roms/sega-genesis-megadrive/street-fighter-ii-special-champion-edition-europe/26496.html). Just put it in a ROMs folder and do `extract here`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SF2 For Gym-Retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retro just allows us to interface with the game ROM through the emulator\n",
    "import retro\n",
    "# Import time so we can control the speed of the game manually\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment For Training\n",
    "\n",
    "### What we're going to do\n",
    "- Preprocess Observations (condense the info passed to the agent) - grayscale, frame delta, resize the frame so we have less pixels.\n",
    "- Filter the action - parameter\n",
    "- Custom reward function. For our purposes we are just going to do the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base environment classe for a wrapper\n",
    "from gym import Env\n",
    "# The space shapes\n",
    "from gym.spaces import MultiBinary, Box\n",
    "# Helper libraries for preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom environment\n",
    "class StreetFighter(Env):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Specify action and observation spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        # Start an instance of the game\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Gets the first frame\n",
    "        obs = self.game.reset()\n",
    "        # Preprocess the data\n",
    "        obs = self.preprocess(obs)\n",
    "        # At the start, just make the prev frame the first frame as well\n",
    "        self.previous_frame = obs\n",
    "        # Create an attribute to hold the score delta\n",
    "        self.score = 0  \n",
    "              \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, obs):\n",
    "        # Grayscale the frame\n",
    "        grayed = cv2.cvtColor(obs, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize the frame\n",
    "        resized = cv2.resize(grayed, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        # Add a color channels value to resized\n",
    "        processed = np.reshape(resized, (84, 84, 1))\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    # Frame-step\n",
    "    def step(self, action):\n",
    "        # Take a step\n",
    "        obs, def_reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        # Frame delta\n",
    "        # frame_delta = obs - self.previous_frame\n",
    "        frame_delta = obs\n",
    "        self.previous_frame = obs\n",
    "        # Reshape the reward function\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages we need to do our reinforcement learning\n",
    "# Pytorch is the base ML framework\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "# Stable-Baselines has a lot of stuff for reinforcement learning specifically\n",
    "!pip install stable-baselines3[extra]\n",
    "# Optuna is used to help us tune our hyperparameters efficiently\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Used for hyper-param optimization\n",
    "import optuna\n",
    "# PPO algorithm for RL\n",
    "from stable_baselines3 import PPO\n",
    "# Used to evaluate how well the model is performing on this environment\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Used for logging\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize our frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "This isn't the actual training, this is just an easy way for us to determine the best hyperparam values for PPO in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'\n",
    "\n",
    "# Function to return the test hyperparams\n",
    "def OptimizePPO(trial: optuna.trial.Trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma': trial.suggest_loguniform('gamma', .8, .9999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-7, 1e-6),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', .1, .4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', .8, .99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop and return mean reward\n",
    "def OptimizeAgent(trial: optuna.trial.Trial):\n",
    "    try:\n",
    "        params = OptimizePPO(trial)\n",
    "\n",
    "        # Create environment\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        \n",
    "        # Create the model\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **params)\n",
    "        # Train. Obviously, more timesteps means better results. 100000 would be the ideal number but\n",
    "        # we will just do 30000 for now so we can see results quickly.\n",
    "        model.learn(total_timesteps=300000, tb_log_name='LR_TEST')\n",
    "        \n",
    "        # Evaluate the model. n_eval_episodes is the number of games the model plays. Should be like 20 to 30.\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        env.close()\n",
    "        \n",
    "        # Save the model so we don't have to do this again\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    \n",
    "    except Exception as e:\n",
    "        if env != None: env.close()\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# Optimize our boy. n_trials would ideally be higher (like 100), but we want results so for now we will do much less (like 10).\n",
    "# Also, if we install the retro wrapper that allows parallelization we could do more than 1 job at a time and\n",
    "# train MUCH faster, so should prob look into that.\n",
    "study.optimize(func=OptimizeAgent, n_trials=5, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best params to use in the actual training. This is super handy!\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best trial we had in our testing\n",
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mu:\\PythonProjects\\SF2_AI\\hadoukenator.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m'\u001b[39m\u001b[39mCnnPolicy\u001b[39m\u001b[39m'\u001b[39m, env, tensorboard_log\u001b[39m=\u001b[39mLOG_DIR, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Train. Obviously, more timesteps means better results. 100000 would be the ideal number but\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# we will just do 30000 for now so we can see results quickly.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m300000\u001b[39;49m, tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLR_TEST\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Evaluate the model. n_eval_episodes is the number of games the model plays. Should be like 20 to 30.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m mean_reward, _ \u001b[39m=\u001b[39m evaluate_policy(model, env, n_eval_episodes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:250\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    247\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    249\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 250\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[0;32m     97\u001b[0m     \u001b[39m# Transpose the terminal observations\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m idx, done \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dones):\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py:33\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\n\u001b[0;32m     31\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     32\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]], np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray, List[Dict[\u001b[39mstr\u001b[39m, Any]],]:\n\u001b[1;32m---> 33\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[0;32m     34\u001b[0m     observations, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstacked_obs\u001b[39m.\u001b[39mupdate(observations, dones, infos)\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\stott\\miniconda3\\envs\\gym_retro\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;32mu:\\PythonProjects\\SF2_AI\\hadoukenator.ipynb Cell 18\u001b[0m in \u001b[0;36mStreetFighter.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# Take a step\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     obs, def_reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# Frame delta\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# frame_delta = obs - self.previous_frame\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     frame_delta \u001b[39m=\u001b[39m obs\n",
      "\u001b[1;32mu:\\PythonProjects\\SF2_AI\\hadoukenator.ipynb Cell 18\u001b[0m in \u001b[0;36mStreetFighter.preprocess\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Grayscale the frame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     grayed \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(obs, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# Resize the frame\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/u%3A/PythonProjects/SF2_AI/hadoukenator.ipynb#X55sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     resized \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(grayed, (\u001b[39m84\u001b[39m, \u001b[39m84\u001b[39m), interpolation\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mINTER_CUBIC)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l_rates = [5e-7, 5e-6, 5e-5, 5e-4, 5e-3]\n",
    "\n",
    "for rate in range(len(l_rates)):\n",
    "    # Investigate learning rates\n",
    "    params = {\n",
    "        'n_steps': 4096,\n",
    "        'gamma': .9,\n",
    "        'learning_rate': l_rates[rate],\n",
    "        'clip_range': .25,\n",
    "        'gae_lambda': .9\n",
    "    }\n",
    "\n",
    "    print(\"Testing l_rates[{}] : {}\".format(rate, l_rates[rate]))\n",
    "\n",
    "    # Create environment\n",
    "    env = StreetFighter()\n",
    "    env = Monitor(env, LOG_DIR)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    \n",
    "    # Create the model\n",
    "    model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **params)\n",
    "    # Train. Obviously, more timesteps means better results. 100000 would be the ideal number but\n",
    "    # we will just do 30000 for now so we can see results quickly.\n",
    "    model.learn(total_timesteps=300000, tb_log_name='LR_TEST')\n",
    "    \n",
    "    # Evaluate the model. n_eval_episodes is the number of games the model plays. Should be like 20 to 30.\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    env.close()\n",
    "    \n",
    "    # Save the model so we don't have to do this again\n",
    "    SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(rate))\n",
    "    model.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load up our best trial and see what he can do with real training! This is why we are saving them.\n",
    "model = PPO.load(os.path.join(OPT_DIR, 'trial_10_best_model.zip'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Our Training Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainingAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "\n",
    "callback = TrainingAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model!\n",
    "### Viewing the performance with tensorboard\n",
    "To do so, just cd into the logs directory and run this: `tensorboard --logdir=.` . Tensorboard will run on a localhost that you can view in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we haven't run a study, use these (we found them to be best)\n",
    "params = {'n_steps': 2570.949, 'gamma': 0.906, 'learning_rate': 2e-07, 'clip_range': 0.369, 'gae_lambda': 0.891}\n",
    "params['n_steps'] = int(np.round(params['n_steps'] / 64)) * 64\n",
    "params['learning_rate'] = 5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best params from the study\n",
    "params = study.best_params\n",
    "# Correct n_steps to be a factor of the batch size (64)\n",
    "params['n_steps'] = int(np.round(study.best_params['n_steps'] / 64)) * 64\n",
    "# Let's use a lower learning rate\n",
    "# params['learning_rate'] = 5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with the optimal params\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Only pick 1 (or no) load to do (duh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our best performing model from hyperparam optimization for a head start\n",
    "model.load(os.path.join(OPT_DIR, 'trial_{}_best_model.zip'.format(int(9))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(os.path.join(OPT_DIR, 'akash','trial_{}_best_model.zip'.format(int(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the best performance we saw.\n",
    "model.load('./train/best_model_1500000.zip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously, more timesteps means better performance. Like 5000000 would be a good number.\n",
    "model.learn(total_timesteps=2000000, callback=callback, reset_num_timesteps=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the best performance we saw.\n",
    "model.load('./train/from_tut/best_model_5460000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well the model works\n",
    "mean_reward, reward_stdev = evaluate_policy(model, env, n_eval_episodes=5, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch Our Boy Work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to close the env before starting a new one.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The game loop\n",
    "num_games = 5\n",
    "# Only play one game\n",
    "for game in range(num_games):\n",
    "    # Resest the env\n",
    "    obs = env.reset()\n",
    "    # Will tell us if we've died or beaten the game\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Take a breather (60fps)\n",
    "        # time.sleep(1/90)\n",
    "        print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
