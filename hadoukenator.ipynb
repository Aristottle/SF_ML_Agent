{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "Only need to do this once for your environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install retro\n",
    "!pip install gym gym-retro\n",
    "\n",
    "# We have to downgrade gym in order to preserve retro's functionality\n",
    "!pip install gym==0.21.0\n",
    "\n",
    "# A library we use to preprocess the frame for training\n",
    "!pip install opencv-python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the ROMs to retro\n",
    "After retro is installed to your environment, you also have to add the Street Fighter ROMs to retro!\n",
    "You do so by navigating to the location of your ROM in the terminal while your environment is activated\n",
    "and running this command:\n",
    "`python -m retro.import .`\n",
    "\n",
    "ROM can be downloaded [here](https://wowroms.com/en/roms/sega-genesis-megadrive/street-fighter-ii-special-champion-edition-europe/26496.html). Just put it in a ROMs folder and do `extract here`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SF2 For Gym-Retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retro just allows us to interface with the game ROM through the emulator\n",
    "import retro\n",
    "# Import time so we can control the speed of the game manually\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use this to see all of the games supported by retro\n",
    "retro.data.list_games()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment For Training\n",
    "\n",
    "### What we're going to do\n",
    "- Preprocess Observations (condense the info passed to the agent) - grayscale, frame delta, resize the frame so we have less pixels.\n",
    "- Filter the action - parameter\n",
    "- Custom reward function. For our purposes we are just going to do the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base environment classe for a wrapper\n",
    "from gym import Env\n",
    "# The space shapes\n",
    "from gym.spaces import MultiBinary, Box\n",
    "# Helper libraries for preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom environment\n",
    "class StreetFighter(Env):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Specify action and observation spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "        # Start an instance of the game\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED)\n",
    "    \n",
    "    def reset(self):\n",
    "        # Gets the first frame\n",
    "        obs = self.game.reset()\n",
    "        # Preprocess the data\n",
    "        obs = self.preprocess(obs)\n",
    "        # At the start, just make the prev frame the first frame as well\n",
    "        self.previous_frame = obs\n",
    "        # Create an attribute to hold the score delta\n",
    "        self.score = 0  \n",
    "              \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, obs):\n",
    "        # Grayscale the frame\n",
    "        grayed = cv2.cvtColor(obs, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize the frame\n",
    "        resized = cv2.resize(grayed, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        # Add a color channels value to resized\n",
    "        processed = np.reshape(resized, (84, 84, 1))\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    # Frame-step\n",
    "    def step(self, action):\n",
    "        # Take a step\n",
    "        obs, def_reward, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        # Frame delta\n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "        # Reshape the reward function\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        \n",
    "        return frame_delta, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreetFighter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The game loop\n",
    "num_games = 1\n",
    "# Only play one game\n",
    "for game in range(num_games):\n",
    "    # Resest the env\n",
    "    obs = env.Reset()\n",
    "    # Will tell us if we've died or beaten the game\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.Render()\n",
    "        obs, reward, done, info = env.Step(env.action_space.sample())\n",
    "        # Take a breather\n",
    "        # time.sleep(1/60)\n",
    "        print(reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Meat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages we need to do our reinforcement learning\n",
    "# Pytorch is the base ML framework\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "# Stable-Baselines has a lot of stuff for reinforcement learning specifically\n",
    "!pip install stable-baselines3[extra]\n",
    "# Optuna is used to help us tune our hyperparameters efficiently\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for hyper-param optimization\n",
    "import optuna\n",
    "# PPO algorithm for RL\n",
    "from stable_baselines3 import PPO\n",
    "# Used to evaluate how well the model is performing on this environment\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Used for logging\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize our frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "This isn't the actual training, this is just an easy way for us to determine the best hyperparam values for PPO in this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'\n",
    "\n",
    "# Function to return the test hyperparams\n",
    "def OptimizePPO(trial: optuna.trial.Trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 8192),\n",
    "        'gamma': trial.suggest_loguniform('gamma', .8, .9999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', .1, .4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', .8, .99)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop and return mean reward\n",
    "def OptimizeAgent(trial: optuna.trial.Trial):\n",
    "    try:\n",
    "        params = OptimizePPO(trial)\n",
    "        # Create environment\n",
    "        env = StreetFighter()\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        \n",
    "        # Create the model\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **params)\n",
    "        # Train. Obviously, more timesteps means better results. 100000 would be the ideal number but\n",
    "        # we will just do 30000 for now so we can see results quickly.\n",
    "        model.learn(total_timesteps=30000)\n",
    "        \n",
    "        # Evaluate the model. n_eval_episodes is the number of games the model plays. Should be like 20 to 30.\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "        \n",
    "        # Save the model so we don't have to do this again\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    \n",
    "    except Exception as e:\n",
    "        env.close()\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# Optimize our boy. n_trials would ideally be higher (like 100), but we want results so for now we will do much less (like 10).\n",
    "# Also, if we install the retro wrapper that allows parallelization we could do more than 1 job at a time and\n",
    "# train MUCH faster, so should prob look into that.\n",
    "study.optimize(func=OptimizeAgent, n_trials=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best params to use in the actual training. This is super handy!\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best trial we had in our testing\n",
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load up our best trial and see what he can do with real training! This is why we are saving them.\n",
    "model = PPO.load(os.path.join(OPT_DIR, 'trial_0_best_model.zip'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Our Training Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainingAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "\n",
    "callback = TrainingAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "params = study.best_params\n",
    "# Correct n_steps to be a factor of the batch size (64)\n",
    "params['n_steps'] = int(np.round(study.best_params['n_steps'] / 64)) * 64\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our best performing model from hyperparam optimization for a head start\n",
    "model = PPO.load(os.path.join(OPT_DIR, 'trial_{}_best_model.zip'.format(int())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously, more timesteps means better performance. Like 5000000 would be a good number.\n",
    "model.learn(total_timesteps=30000, callback=callback)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the best performance we saw.\n",
    "model = PPO.load('./train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, reward_stdev = evaluate_policy(model, env, n_eval_episodes=5, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch Our Boy Work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The game loop\n",
    "num_games = 1\n",
    "# Only play one game\n",
    "for game in range(num_games):\n",
    "    # Resest the env\n",
    "    obs = env.reset()\n",
    "    # Will tell us if we've died or beaten the game\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # Take a breather (60fps)\n",
    "        time.sleep(1/60)\n",
    "        print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
